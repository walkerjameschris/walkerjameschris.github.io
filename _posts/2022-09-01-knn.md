---
title: "Implementing KNN In Python"
date: "2022-09-01"
---

K-nearest neighbors (KNN) is an algorithm which identifies the k
nearest data points in a training sample to a new observation. Typically,
nearest is defined by the Euclidian (or straight line) distance, however,
other distance norms can be used.

Python is already home to several KNN implementations the most famous of
which is  the scikit-learn implementation. I still believe there is value
in writing your own model implementations to learn more about how they work.

First lets break down what KNN is doing visually and then code up our own
implementation. The visual below (built using D3.js) shows several points
which are classified into the red and blue groups. 

You can hover your mouse over this visual to develop an understanding of
how the nearest three points impact the classification of the point.

<div id="d3-chart-knn"></div>
<script>
function d3_chart_knn() {
    // Define function to return mode of array
    function mode(arr) {
        counts = {};
        arr.forEach((d) => {
            if (Object.keys(counts).includes(d)) {
                counts[d] = counts[d] + 1;
            } else {
                counts[d] = 1;
            }
        })
        var sort = Object.entries(counts).sort((a, b) => b[1] - a[1]);
        return sort[0][0];
    }

    //// BEGIN SVG SETUP ////

    // Determine width of container
    var div_spec = d3.select("#d3-chart-knn").node().getBoundingClientRect();

    // Set margins for D3
    var margin = 50;
    var width = +div_spec.width - (margin * 2);
    var height = 500 - (margin * 2);

    // Append SVG to DOM
    var svg_knn = d3.select("#d3-chart-knn")
        .append("svg")
        .attr("width", width + (margin * 2))
        .attr("height", height + (margin * 2));

    var plot_knn = svg_knn.append("g")
        .attr("id", "container")
        .attr("width", width)
        .attr("height", height)
        .attr("transform", "translate(" + margin + "," + margin + ")");

    //// DEFINE DATA AND AXES ////

    // Points and classifications
    const k_near = 3; data = [];
    var xpt = [1.1, 1.6, 1.0, 1.2, 2.0, 2.6, 4.2, 4.0, 3.5, 2.8, 3.5, 4.0]
    var ypt = [1.3, 1.6, 3.2, 2.4, 1.0, 0.8, 2.7, 4.0, 4.4, 4.4, 3.2, 2.2];
    var cls = ["#de5950", "#de5950", "#de5950", "#de5950", "#de5950", "#de5950",
        "#506ebf", "#506ebf", "#506ebf", "#506ebf", "#506ebf", "#506ebf"];

    // Transform data into D3 friendly shape
    cls.forEach((v, i) => data.push({ x: xpt[i], y: ypt[i], c: v }));

    // Create line plotting function
    var line_fun_knn = d3.line()
        .x((d) => d.x)
        .y((d) => d.y);

    // Create x axis function
    var x_fun_knn = d3.scaleLinear()
        .domain([0, 5])
        .range([0, width]);

    // Create y axis function
    var y_fun_knn = d3.scaleLinear()
        .domain([0, 5])
        .range([height, 0]);

    // Append x-axis to DOM
    plot_knn.append("g")
        .attr("id", "x-axis")
        .attr("transform", "translate(0," + height + ")")
        .call(d3.axisBottom(x_fun_knn));

    // Append y-axis to DOM
    plot_knn.append("g")
        .attr("id", "y-axis")
        .call(d3.axisLeft(y_fun_knn));

    //// PLOT POINTS AND DEFINE EVENT HANDLING ////

    // Lines for k-nearest neighbors
    var lines_knn = plot_knn.append("g").attr("id", "k-lines");

    // Plot static points with classification
    plot_knn.append("g")
        .attr("id", "circles")
        .selectAll("circle")
        .data(data)
        .enter()
        .append("circle")
        .attr("cx", function (d) { return x_fun_knn(d.x); })
        .attr("cy", function (d) { return y_fun_knn(d.y); })
        .attr("r", 10)
        .style("stroke", "black")
        .style("fill", (d) => d.c);

    // Add dynamic circle
    var mv_circle_knn = plot_knn.append("g")
        .attr("id", "mv-circle")
        .append("circle")
        .attr("cx", x_fun_knn(2.5))
        .attr("cy", y_fun_knn(2.5))
        .attr("r", 10)
        .style("stroke", "gold")
        .attr("stroke-width", 3)
        .style("fill", "none");

    // Event handling rectangle
    // append the rectangle to capture mouse
    plot_knn.append("rect")
        .attr("id", "mouse-capture")
        .attr("width", width)
        .attr("height", height)
        .style("fill", "none")
        .style("pointer-events", "all")
        .on("mousemove", function () {
            // Determine x and y position
            var x_pos = event.offsetX - margin;
            var y_pos = event.offsetY - margin;
            var point_pos = { x: x_pos, y: y_pos };

            // Determine distance between all points
            var dist = [];
            data.forEach(function (d) {
                x_cor = (x_pos - x_fun_knn(d.x)) ** 2;
                y_cor = (y_pos - y_fun_knn(d.y)) ** 2;
                dist.push(x_cor + y_cor);
            });

            // Get three closest indices and classes
            var sorted = Object.entries(dist).sort((a, b) => a[1] - b[1]);
            var k_ind = sorted.slice(0, k_near).map((d) => +d[0]);
            var classes = k_ind.map((d) => cls[d]);

            // Clear line container for new paths
            lines_knn.selectAll("*").remove();

            // Create lines for k nearest
            k_ind.forEach(function (ind) {
                var k_data = [point_pos, { x: x_fun_knn(xpt[ind]), y: y_fun_knn(ypt[ind]) }];
                lines_knn.append("g")
                    .attr("id", "ind" + ind)
                    .append("path")
                    .attr("class", "line")
                    .attr("d", line_fun_knn(k_data))
                    .style("fill", "none")
                    .style("stroke", "black");
            })

            // Move point and color based on classification
            mv_circle_knn.attr("cx", x_pos)
                .attr("cy", y_pos)
                .style("fill", mode(classes));
        })
        .on("mouseout", function () {
            // Clear line container
            lines_knn.selectAll("*").remove();

            // Move point and color to default
            mv_circle_knn.attr("cx", x_fun_knn(2.5))
                .attr("cy", y_fun_knn(2.5))
                .style("fill", "none")
        });
}

d3_chart_knn();
</script>

We can identify the three (k = 3) closest points and determine of those,
which classification is the most common. The most common classification
becomes our predicted value.

A few notes before we jump into our own implementation. First, it is common
to use an odd number for k when performing classification to avoid ties.
Second, one downside of KNN when compared to other models is that KNN must
be packaged with the training data to make predictions. This is different
than linear regression which only requires the coefficients to be known at
the time of prediction, for example.

Now let's look at my implementation of KNN in Python. Only 8 lines of code
(excluding function imports)! A safer version of this code may also include
several assertion checks to ensure inputs are of the expected type and shape.

```python
import numpy as np
import scipy as sci

def knn(new, train, labels, k=3, mode="c"):

    distances = np.sum((new - train) ** 2, axis=1)
    k_closest = distances.argsort()[:k]
    values = np.take(labels, k_closest)
    
    if mode == "c":
        return sci.stats.mode(values)[0][0]
        
    if mode == "r":
        return np.mean(values)
```

Lets look at this function line by line. First, I define a function called
`knn` which accepts a singular new observation called new, the training data
called train with its associated labels (the correct prediction), and the mode
which is either c for classification or r for regression.

```python
def knn(new, train, labels, k=3, mode="c")
```

From there I compute how far each of the training points is from the new observation.
To accurately compute the distances you would need to take the square root of this
value. However, because we are only interested in the rank ordering of points, we
can skip that step.

```python
distances = np.sum((new - train) ** 2, axis=1)
```

Next I use argsort and take from numpy to rank order the indices by how close they
are to the new observation. I use index slicing to grab the k nearest points. From
there I use take to grab the values of the k closest indices from the label data.

```python
k_closest = distances.argsort()[:k]
values = np.take(labels, k_closest)
```

Finally, I take the mode of the values for classification or the mean for regression.
To predict over multiple observations I could pass the function into a list comprehension.

```
[knn(i, train, labels) for i in test]
```

This was a simple overview of KNN regression using basic numpy and scipy functions!
